{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import GridSearchCV,StratifiedKFold\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pickle\n",
    "##### scale your data ######\n",
    "##### MinMaxScaler: This estimator scales and translates each feature individually into (0,1) #######\n",
    "##### StandardScaler: Standardize features by removing the mean and scaling to unit variance #####\n",
    "##### RobustScaler: Scale features using statistics that are robust to outliers ######\n",
    "#scaler = [MinMaxScaler(),StandardScaler(), RobustScaler()]\n",
    "scaler = [StandardScaler()]\n",
    "from sklearn.utils import resample\n",
    "#Get data loaded\n",
    "df=pd.read_csv(\"df_feat_small.csv\").drop('Unnamed: 0',axis=1).dropna().copy()\n",
    "#df=df.drop('P/B_RATIO_Q_0',axis=1)\n",
    "list_etf = []\n",
    "df_etf = df[['DWAS', 'EWSC',\n",
    "       'FNDA', 'IUSS', 'OMFS', 'PBSM', 'PSCD', 'PSCF', 'PSCH', 'PSCI', 'PSCT',\n",
    "       'PSCU', 'RWJ', 'RZG', 'RZV', 'SLY', 'SLYG', 'SLYV', 'SMLV', 'SPMD',\n",
    "       'SPSM', 'VB', 'VBK', 'VBR', 'VIOG', 'VIOO', 'VIOV', 'VSS', 'XSHD',\n",
    "       'XSHQ', 'XSMO', 'XSVM']].copy()\n",
    "###### Create list of ETFs\n",
    "ETF_list=df_etf.columns\n",
    "df_features = df.drop(ETF_list, axis=1)\n",
    "feature_list=df_features.columns\n",
    "n=ETF_list.size\n",
    "###### Define a Function that Balances Scale Dataset\n",
    "def upSample(df):\n",
    "    n=ETF_list.size # number of ETFs\n",
    "    Dataset_X=[]\n",
    "    Dataset_y=[]\n",
    "    X_train, X_test, y_train, y_test = [], [], [], []\n",
    "    #Dataset_X=np.array([] for j in range(n))\n",
    "    #Dataset_y = np.array([] for j in range(n))\n",
    "    for i in range(n):\n",
    "        #Count the number of ones and zeros\n",
    "        numbFalse = (df[ETF_list[i]] == 0).sum()\n",
    "        numbTrue = (df[ETF_list[i]] == 1).sum()\n",
    "        if numbFalse > numbTrue:\n",
    "            upSample = numbFalse\n",
    "        else:\n",
    "            upSample = numbTrue\n",
    "        # Separate majority and minority classes\n",
    "        df_majority = df[df[ETF_list[i]] == 0]\n",
    "        df_minority = df[df[ETF_list[i]] == 1]\n",
    "        # Upsample minority class\n",
    "        df_minority_upsampled = resample(df_minority,\n",
    "                                             replace=True,     # sample with replacement\n",
    "                                             n_samples = upSample,    # to match majority class\n",
    "                                             random_state=123) # reproducible results\n",
    "        df_upsampled = pd.concat([df_majority, df_minority_upsampled])\n",
    "        # Separate input features (X) and target variable (y)\n",
    "        y = df_upsampled[ETF_list[i]]\n",
    "        X = df_upsampled.drop(ETF_list[i], axis=1)\n",
    "        X_train_temp, X_test_temp, y_train_temp, y_test_temp = train_test_split(X, y, test_size = 0.3, random_state = 1)\n",
    "        #print(y.size)\n",
    "        X_train.append(X_train_temp)\n",
    "        X_test.append(X_test_temp)\n",
    "        y_train.append(y_train_temp)\n",
    "        y_test.append(y_test_temp)\n",
    "        #Dataset_X.append(X)\n",
    "        #Dataset_y.append(y)\n",
    "    #X_train, X_test, y_train, y_test = train_test_split(Dataset_X, Dataset_y, test_size = 0.3, random_state = 1)\n",
    "    #result=X_train, X_test, y_train, y_test\n",
    "    return X_train, X_test, y_train, y_test\n",
    "###### generate dataset #####\n",
    "X_train, X_test, y_train, y_test=upSample(df)\n",
    "###### use Random Forest as classifier #####\n",
    "clf=RandomForestClassifier(random_state=0)\n",
    "###### cross validation: separate dataset ##########\n",
    "kfold= StratifiedKFold(n_splits=3, shuffle = True, random_state=0)\n",
    "####set up pipline here ######\n",
    "#### try different scaler here try [MinMaxScaler(),StandardScaler(), RobustScaler()] ####\n",
    "steps = [('scaler', StandardScaler()),('rf',clf)]\n",
    "pipeline= Pipeline(steps)\n",
    "######## grid search for hyperparameter #########\n",
    "######## Need to tune parameter, max_depth, min_samples_leaf and max_features #####\n",
    "parameters = {\n",
    "    #'rf__scaler': scaler,\n",
    "    'rf__max_depth':  [3,4],#np.linspace(2,4,2),\n",
    "    'rf__min_samples_leaf': [3,4],#np.linspace(1,5,5),\n",
    "    'rf__max_features': [5,6,7],#np.linspace(2,4,2),\n",
    "    #\"rf__criterion\": [\"gini\", \"entropy\"]\n",
    "}\n",
    "##### model ####\n",
    "mdl=GridSearchCV(pipeline,param_grid = parameters,n_jobs=-1,cv=kfold,scoring='roc_auc')\n",
    "##### fit model with data #####\n",
    "##### Here we generate a matrix , 32 columns gives predicted result of 32 classifier ######\n",
    "Predict_M=[]\n",
    "for i in range(n):\n",
    "    cur_df=X_train[i]\n",
    "    num_of_features = cur_df.columns.size - n\n",
    "    X=cur_df[cur_df.columns[-num_of_features:]]\n",
    "    Y=y_train[i]\n",
    "    X=X.drop('TICKER',axis=1)\n",
    "    mdl.fit(X,Y)\n",
    "    result=mdl.predict(X)\n",
    "    Predict_M.append(result)\n",
    "    filename = 'model'+str(i+1)+'sav'\n",
    "    pickle.dump(mdl, open(filename, 'wb'))\n",
    "###### Test the model ######\n",
    "###### Load models by pickle #####\n",
    "Test_M=[]\n",
    "for i in range(n):\n",
    "    cur_df=X_test[i]\n",
    "    num_of_features = cur_df.columns.size - n\n",
    "    X_cur_test=cur_df[cur_df.columns[-num_of_features:]]\n",
    "    #X_cur_test = X_test[i]\n",
    "    y_cur_test = y_test[i]\n",
    "    filename = 'model'+str(i+1)+'sav'\n",
    "    mdl = pickle.load(open(filename, 'rb'))\n",
    "    X_cur_test=X_cur_test.drop('TICKER',axis=1)\n",
    "    result= mdl.predict(X_cur_test)\n",
    "    Test_M.append(result)\n",
    "    \n",
    "##### Accuracy\n",
    "diff=0\n",
    "total=0\n",
    "for i in range(len(y_test)):\n",
    "    total+=len(y_test[i])\n",
    "    diff+=np.sum(abs(y_test[i]*1-Test_M[i].T*1))\n",
    "Accuracy = 1-diff/total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
